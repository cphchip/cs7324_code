{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs7324 Lab 6 - Sequential Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chip Henderson - 48996654 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In this lab, I'll be using a collection of Tweets related to the coronavirus pandemic. My intent is to use Convolutional Neural Network and Transformer models to analyze sentiment.\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import and Tokenization\n",
    "\n",
    "My dataset is already split into training and testing categories. Since I will need to split out my y values as well, I'm going to combine the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 44955 entries, 0 to 3797\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       44955 non-null  int64 \n",
      " 1   ScreenName     44955 non-null  int64 \n",
      " 2   Location       35531 non-null  object\n",
      " 3   TweetAt        44955 non-null  object\n",
      " 4   OriginalTweet  44955 non-null  object\n",
      " 5   Sentiment      44955 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# I'm using a latin enoding because there are some special characters\n",
    "X_train = pd.read_csv(r'C:\\Users\\Chip\\source\\repos\\cs7324_code\\Data_Sources\\Coronavirus_tweets\\Corona_NLP_train.csv',encoding='iso-8859-1')\n",
    "X_test = pd.read_csv(r'C:\\Users\\Chip\\source\\repos\\cs7324_code\\Data_Sources\\Coronavirus_tweets\\Corona_NLP_test.csv',encoding='iso-8859-1')\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the memory footprint and compute times for some of these networks, I'm going to make my dataset as simple as possible. Also, I'm focused on performing a many-to-one analysis. Therefore, I'll be dropping everything except for the Original Tweet and Sentiment columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of                                           OriginalTweet           Sentiment\n",
       "0     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1     advice Talk to your neighbours family to excha...            Positive\n",
       "2     Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3     My food stock is not the only one which is emp...            Positive\n",
       "4     Me, ready to go at supermarket during the #COV...  Extremely Negative\n",
       "...                                                 ...                 ...\n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive\n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative\n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral\n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative\n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive\n",
       "\n",
       "[44955 rows x 2 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep = ['OriginalTweet', 'Sentiment']\n",
    "features_to_drop = [feature for feature in X.columns if feature not in features_to_keep]\n",
    "\n",
    "# Drop features I won't be using for this lab\n",
    "X_reduced = X.drop(features_to_drop,axis=1)\n",
    "\n",
    "# Determine number of instances \n",
    "X_reduced.value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance of the examples above. There are some characters that I'm not sure I want to include in my embeddings. Some are stop words like \"a,\" \"the,\" \"is,\" \"are.\" Others are @tags that I don't believe will help my sentiment predictions. When I tokenize these Tweets I'm going to remove the stop words and will examine some differences in leaving the @tags in or removing them.\n",
    "\n",
    "First, I'll go ahead and split my dataset so that I'm not tokenizing my target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8',\n",
       "       'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order',\n",
       "       'Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P',\n",
       "       ...,\n",
       "       \"Asst Prof of Economics @cconces was on @NBCPhiladelphia talking about her recent research on coronavirus' impact on the economy. Watch it here (starting at :33): https://t.co/8tfYNoro5l\",\n",
       "       \"Gov need to do somethings instead of biar je rakyat assume 'lockdown' ke or even worst. Harini semua supermarket crowded like hell. Lagi mudah virus tu tersebar ?? #COVID2019\",\n",
       "       'I and @ForestandPaper members are committed to the safety of our employees and our end-users. We are monitoring COVID-19. Rest assured that tissue manufacturers are continuing to produce and ship products.  https://t.co/qF6hclCAEq https://t.co/xyvbNsFeXA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_reduced.OriginalTweet.values\n",
    "y = X_reduced.Sentiment.values\n",
    "\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
